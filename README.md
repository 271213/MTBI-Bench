# MTBI-Bench: A Multimodal Benchmark for Teacher Behavior Interpretation

The integration of large language models (LLMs) with visual encoders has opened new frontiers in multimodal reasoning for educational applications. However, existing evaluation benchmarks in this domain exhibit three critical limitations:
1. **Result-oriented evaluation paradigms**
2. **Inadequate scale of multimodal data**
3. **Lack of fine-grained reasoning process and diverse teaching behavior annotation**

To address these challenges, we present **MTBI-Bench**, a comprehensive multimodal benchmark specifically designed for evaluating teacher behavior interpretation in varied classroom scenarios.

## Key Contributions

- **Large-Scale Dataset**: A scenario-specific, multi-task, and multimodal dataset capturing diverse teaching behaviors.
- **Human-LLM Collaborative Pipeline**: A three-stage workflow ensuring high-quality multimodal alignment through collaborative human and model annotation.
- **Comprehensive Evaluation**: Empirical analysis of mainstream video LLMs across three core tasks:
  - Teaching behavior recognition
  - Spatial location extraction
  - Video description generation

## Summary of Findings

Our experiments demonstrate that current models still struggle with:
- Recognizing similar or subtle teaching behaviors
- Extracting accurate time boundaries
- Capturing detailed spatiotemporal and semantic information

These limitations highlight concrete directions for future research in multimodal educational AI systems.

